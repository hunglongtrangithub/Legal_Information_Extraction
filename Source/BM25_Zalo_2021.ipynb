{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcX65udZab22",
        "outputId": "6f86a720-6926-4de8-cb58-283e1e64429f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmOziTz6FoJd"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "\"\"\"\n",
        "All of these algorithms have been taken from the paper:\n",
        "Trotmam et al, Improvements to BM25 and Language Models Examined\n",
        "Here we implement all the BM25 variations mentioned. \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class BM25:\n",
        "    def __init__(self, corpus, tokenizer=None):\n",
        "        self.corpus_size = len(corpus)\n",
        "        self.avgdl = 0\n",
        "        self.doc_freqs = []\n",
        "        self.idf = {}\n",
        "        self.doc_len = []\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        if tokenizer:\n",
        "            corpus = self._tokenize_corpus(corpus)\n",
        "\n",
        "        nd = self._initialize(corpus)\n",
        "        self._calc_idf(nd)\n",
        "\n",
        "    def _initialize(self, corpus):\n",
        "        nd = {}  # word -> number of documents with word\n",
        "        num_doc = 0\n",
        "        for document in corpus:\n",
        "            self.doc_len.append(len(document))\n",
        "            num_doc += len(document)\n",
        "\n",
        "            frequencies = {}\n",
        "            for word in document:\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 0\n",
        "                frequencies[word] += 1\n",
        "            self.doc_freqs.append(frequencies)\n",
        "\n",
        "            for word, freq in frequencies.items():\n",
        "                try:\n",
        "                    nd[word]+=1\n",
        "                except KeyError:\n",
        "                    nd[word] = 1\n",
        "\n",
        "        self.avgdl = num_doc / self.corpus_size\n",
        "        return nd\n",
        "\n",
        "    def _tokenize_corpus(self, corpus):\n",
        "        pool = Pool(cpu_count())\n",
        "        tokenized_corpus = pool.map(self.tokenizer, corpus)\n",
        "        return tokenized_corpus\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_batch_scores(self, query, doc_ids):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_top_n(self, query, documents, n=5):\n",
        "\n",
        "        assert self.corpus_size == len(documents), \"The documents given don't match the index corpus!\"\n",
        "\n",
        "        scores = self.get_scores(query)\n",
        "        top_n = np.argsort(scores)[::-1][:n]\n",
        "        return [documents[i] for i in top_n]\n",
        "\n",
        "\n",
        "class BM25Okapi(BM25):\n",
        "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.epsilon = epsilon\n",
        "        super().__init__(corpus, tokenizer)\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        \"\"\"\n",
        "        Calculates frequencies of terms in documents and in corpus.\n",
        "        This algorithm sets a floor on the idf values to eps * average_idf\n",
        "        \"\"\"\n",
        "        # collect idf sum to calculate an average idf for epsilon value\n",
        "        idf_sum = 0\n",
        "        # collect words with negative idf to set them a special epsilon value.\n",
        "        # idf can be negative if word is contained in more than half of documents\n",
        "        negative_idfs = []\n",
        "        for word, freq in nd.items():\n",
        "            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
        "            self.idf[word] = idf\n",
        "            idf_sum += idf\n",
        "            if idf < 0:\n",
        "                negative_idfs.append(word)\n",
        "        self.average_idf = idf_sum / len(self.idf)\n",
        "\n",
        "        eps = self.epsilon * self.average_idf\n",
        "        for word in negative_idfs:\n",
        "            self.idf[word] = eps\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        \"\"\"\n",
        "        The ATIRE BM25 variant uses an idf function which uses a log(idf) score. To prevent negative idf scores,\n",
        "        this algorithm also adds a floor to the idf value of epsilon.\n",
        "        See [Trotman, A., X. Jia, M. Crane, Towards an Efficient and Effective Search Engine] for more info\n",
        "        :param query:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        score = np.zeros(self.corpus_size)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "        for q in query:\n",
        "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
        "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
        "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
        "        return score\n",
        "\n",
        "    def get_batch_scores(self, query, doc_ids):\n",
        "        \"\"\"\n",
        "        Calculate bm25 scores between query and subset of all docs\n",
        "        \"\"\"\n",
        "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
        "        score = np.zeros(len(doc_ids))\n",
        "        doc_len = np.array(self.doc_len)[doc_ids]\n",
        "        for q in query:\n",
        "            q_freq = np.array([(self.doc_freqs[di].get(q) or 0) for di in doc_ids])\n",
        "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
        "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
        "        return score.tolist()\n",
        "\n",
        "\n",
        "class BM25L(BM25):\n",
        "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, delta=0.5):\n",
        "        # Algorithm specific parameters\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.delta = delta\n",
        "        super().__init__(corpus, tokenizer)\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        for word, freq in nd.items():\n",
        "            idf = math.log(self.corpus_size + 1) - math.log(freq + 0.5)\n",
        "            self.idf[word] = idf\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        score = np.zeros(self.corpus_size)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "        for q in query:\n",
        "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
        "            ctd = q_freq / (1 - self.b + self.b * doc_len / self.avgdl)\n",
        "            score += (self.idf.get(q) or 0) * q_freq * (self.k1 + 1) * (ctd + self.delta) / \\\n",
        "                     (self.k1 + ctd + self.delta)\n",
        "        return score\n",
        "\n",
        "    def get_batch_scores(self, query, doc_ids):\n",
        "        \"\"\"\n",
        "        Calculate bm25 scores between query and subset of all docs\n",
        "        \"\"\"\n",
        "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
        "        score = np.zeros(len(doc_ids))\n",
        "        doc_len = np.array(self.doc_len)[doc_ids]\n",
        "        for q in query:\n",
        "            q_freq = np.array([(self.doc_freqs[di].get(q) or 0) for di in doc_ids])\n",
        "            ctd = q_freq / (1 - self.b + self.b * doc_len / self.avgdl)\n",
        "            score += (self.idf.get(q) or 0) * q_freq * (self.k1 + 1) * (ctd + self.delta) / \\\n",
        "                     (self.k1 + ctd + self.delta)\n",
        "        return score.tolist()\n",
        "\n",
        "\n",
        "class BM25Plus(BM25):\n",
        "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, delta=1):\n",
        "        # Algorithm specific parameters\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.delta = delta\n",
        "        super().__init__(corpus, tokenizer)\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        for word, freq in nd.items():\n",
        "            idf = math.log((self.corpus_size + 1) / freq)\n",
        "            self.idf[word] = idf\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        score = np.zeros(self.corpus_size)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "        for q in query:\n",
        "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
        "            score += (self.idf.get(q) or 0) * (self.delta + (q_freq * (self.k1 + 1)) /\n",
        "                                               (self.k1 * (1 - self.b + self.b * doc_len / self.avgdl) + q_freq))\n",
        "        return score\n",
        "\n",
        "    def get_batch_scores(self, query, doc_ids):\n",
        "        \"\"\"\n",
        "        Calculate bm25 scores between query and subset of all docs\n",
        "        \"\"\"\n",
        "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
        "        score = np.zeros(len(doc_ids))\n",
        "        doc_len = np.array(self.doc_len)[doc_ids]\n",
        "        for q in query:\n",
        "            q_freq = np.array([(self.doc_freqs[di].get(q) or 0) for di in doc_ids])\n",
        "            score += (self.idf.get(q) or 0) * (self.delta + (q_freq * (self.k1 + 1)) /\n",
        "                                               (self.k1 * (1 - self.b + self.b * doc_len / self.avgdl) + q_freq))\n",
        "        return score.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXPBC3OzKAXn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def tokenize(n_gram, sentence):\n",
        "    sentence = sentence.split()\n",
        "    to_ret = []\n",
        "    for i in range(len(sentence)-n_gram):\n",
        "        to_ret.append(\" \".join(sentence[i:i+n_gram]))\n",
        "    return to_ret\n",
        "def multitokenize(n_grams, sentence):\n",
        "    sentence = sentence.split()\n",
        "    to_ret = []\n",
        "    for n_gram in n_grams:\n",
        "        for i in range(len(sentence)-n_gram):\n",
        "            to_ret.append(\" \".join(sentence[i:i+n_gram]))\n",
        "\n",
        "    return to_ret\n",
        "\n",
        "docs_list = []\n",
        "keep_track = {}\n",
        "corpus_label = []\n",
        "data_path = \"/content/drive/MyDrive/Zalo 2021/legal_corpus.json\"\n",
        "with open(data_path) as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "\n",
        "for passage in data:\n",
        "    for article in passage['articles']:\n",
        "        docs_list.append(multitokenize([1], article['title']+article['text']))\n",
        "        keep_track[passage['law_id'] + \"###\" + article['article_id']] = article\n",
        "        corpus_label.append(passage['law_id'] + \"###\" + article['article_id'])\n",
        "\n",
        "bm25 = BM25Plus(docs_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAiAqE9xLAeK"
      },
      "outputs": [],
      "source": [
        "def test_bm25(bm25, corpus, questions, top_k):\n",
        "    count = 0\n",
        "    true_positive = 0\n",
        "    for question in questions:\n",
        "        if count % 50 == 0:\n",
        "            print(\"{} complete\".format(count))\n",
        "        count += 1\n",
        "        ques = question[0]\n",
        "        label = question[1]\n",
        "        # print(ques)\n",
        "        top_k_docs = bm25.get_top_n(multitokenize([1], ques), corpus, n=top_k)\n",
        "        for doc in top_k_docs:\n",
        "            if doc == label:\n",
        "                true_positive += 1\n",
        "    return true_positive / len(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvOcjD1HK5-R"
      },
      "outputs": [],
      "source": [
        "questions = []\n",
        "data_path = \"/content/drive/MyDrive/Zalo 2021/train_question_answer.json\"\n",
        "with open(data_path) as json_file:\n",
        "    questions_json = json.load(json_file)\n",
        "for question in questions_json['items']:\n",
        "    label = question[\"relevant_articles\"][0]['law_id'] + \"###\" + question[\"relevant_articles\"][0]['article_id']\n",
        "    q= question['question']\n",
        "    questions.append([q, label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UNDQPhbLj07",
        "outputId": "acaa28db-ff0f-4787-8ae1-7e643d915132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 complete\n",
            "50 complete\n",
            "100 complete\n",
            "150 complete\n",
            "200 complete\n",
            "250 complete\n",
            "300 complete\n",
            "350 complete\n",
            "400 complete\n",
            "450 complete\n",
            "500 complete\n",
            "550 complete\n",
            "600 complete\n",
            "650 complete\n",
            "700 complete\n",
            "750 complete\n",
            "800 complete\n",
            "850 complete\n",
            "900 complete\n",
            "950 complete\n",
            "1000 complete\n",
            "1050 complete\n",
            "1100 complete\n",
            "1150 complete\n",
            "1200 complete\n",
            "1250 complete\n",
            "1300 complete\n",
            "1350 complete\n",
            "1400 complete\n",
            "1450 complete\n",
            "1500 complete\n",
            "1550 complete\n",
            "1600 complete\n",
            "1650 complete\n",
            "1700 complete\n",
            "1750 complete\n",
            "1800 complete\n",
            "1850 complete\n",
            "1900 complete\n",
            "1950 complete\n",
            "2000 complete\n",
            "2050 complete\n",
            "2100 complete\n",
            "2150 complete\n",
            "2200 complete\n",
            "2250 complete\n",
            "2300 complete\n",
            "2350 complete\n",
            "2400 complete\n"
          ]
        }
      ],
      "source": [
        "# questions = questions[:200]\n",
        "test_bm25(bm25, corpus_label, questions, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By2JtVYdCc8q"
      },
      "source": [
        "# **Extracting Training Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oJiF0_lHvRa"
      },
      "outputs": [],
      "source": [
        "def extract_top_k_for_training(bm25, corpus, question, top_k):\n",
        "    ques = question\n",
        "    # print(ques)\n",
        "    top_k_docs = bm25.get_top_n(multitokenize([1], ques), corpus, n=top_k)\n",
        "    to_ret_list = []\n",
        "    for score in top_k_docs:\n",
        "        article = {}\n",
        "        article['law_id'] = score.split(\"###\")[0]\n",
        "        article['article_id'] = score.split(\"###\")[1]\n",
        "        article['title'] = keep_track[score]['title']\n",
        "        article['text'] = keep_track[score]['text']\n",
        "        to_ret_list.append(article)\n",
        "\n",
        "    return to_ret_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-JdRQVlJ76B",
        "outputId": "1e8fb73c-e077-4b8c-a8fe-fc87f2479cf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3196\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3196"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "data_path = '/content/drive/MyDrive/Zalo 2021/train_question_answer.json'\n",
        "with open(data_path) as json_file:\n",
        "    questions = json.load(json_file)\n",
        "count_ques = 0\n",
        "for question in questions['items']:\n",
        "    count_ques += 1\n",
        "print(count_ques)\n",
        "len(questions['items'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poEINaqFCn1Y"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/Zalo 2021/legal_corpus.json'\n",
        "with open(data_path) as json_file:\n",
        "    corpus = json.load(json_file)\n",
        "import random\n",
        "to_save = []\n",
        "count = 0\n",
        "false_ques_count, true_ques_count = 0, 0\n",
        "for question in questions['items']:\n",
        "    if count %50 == 0:\n",
        "        print(\"{} completed\".format(count))\n",
        "    count += 1\n",
        "    for article in corpus:\n",
        "        if article['law_id'] == question['relevant_articles'][0]['law_id']:\n",
        "            for passage in article['articles']:\n",
        "                false_ques = {}\n",
        "                if passage['article_id'] == question['relevant_articles'][0]['article_id']:\n",
        "                    if len(passage['text'].split()) < 400:\n",
        "                        true_ques_count += 1\n",
        "                                                                            \n",
        "                        true_ques = {}\n",
        "                        true_ques['question'] = question['question']\n",
        "                        true_ques['article'] = passage\n",
        "                        true_ques['label'] = 1\n",
        "                        to_save.append(true_ques)\n",
        "                        break\n",
        "    \n",
        "    negative_sample_list = extract_top_k_for_testing(bm25, corpus_label, question['question'], 10)\n",
        "    for article in negative_sample_list:\n",
        "        if article['article_id'] != question['relevant_articles'][0]['article_id'] or article['law_id'] != question['relevant_articles'][0]['law_id']:\n",
        "            if random.random() < 0.2:\n",
        "                if len(article['text'].split()) < 400:\n",
        "                    false_ques_count += 1\n",
        "                    save_article ={}\n",
        "                    false_ques = {}\n",
        "                    false_ques['question'] = question['question']\n",
        "                    false_ques['article'] = article\n",
        "                    false_ques['label'] = 0\n",
        "                    to_save.append(false_ques)\n",
        "                \n",
        "print(true_ques_count, false_ques_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG6mqG_WJXZ9"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "import os\n",
        "dest_path = '/content/drive/MyDrive/Zalo 2021/Training Data'\n",
        "today = 'training_draft_bm25_vers_short.json'\n",
        "\n",
        "file_path = os.path.join(dest_path, today)\n",
        "with codecs.open(file_path, \"w\", encoding='utf8') as outfile:\n",
        "        json.dump(to_save , outfile, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn8NtYLwhMgf"
      },
      "source": [
        "# **Public Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMYm-eYDueV6"
      },
      "outputs": [],
      "source": [
        "def extract_top_k_for_testing(bm25, corpus, question, top_k):\n",
        "    ques = question\n",
        "    # print(ques)\n",
        "    top_k_docs = bm25.get_top_n(multitokenize([1], ques), corpus, n=top_k)\n",
        "    to_ret_list = []\n",
        "    for score in top_k_docs:\n",
        "        if len(keep_track[score]['text'].split()) < 400: \n",
        "            article = {}\n",
        "            article['law_id'] = score.split(\"###\")[0]\n",
        "            article['article_id'] = score.split(\"###\")[1]\n",
        "            article['title'] = keep_track[score]['title']\n",
        "            article['text'] = keep_track[score]['text']\n",
        "            to_ret_list.append(article)\n",
        "        else:\n",
        "            para_list = [multitokenize([1], para) for para in keep_track[score]['text'].split('\\n')]\n",
        "            smallbm25 = BM25Plus(para_list)\n",
        "            top_paras = smallbm25.get_top_n(multitokenize([1], ques), para_list, n=15)\n",
        "            final_article = \"\\n\".join([\" \".join(para) for para in top_paras])\n",
        "            article = {}\n",
        "            article['law_id'] = score.split(\"###\")[0]\n",
        "            article['article_id'] = score.split(\"###\")[1]\n",
        "            article['title'] = keep_track[score]['title']\n",
        "            article['text'] = final_article\n",
        "            to_ret_list.append(article)\n",
        "\n",
        "\n",
        "    return to_ret_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ordv5pIuLHwm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "test = []\n",
        "data_path = \"/content/drive/MyDrive/Zalo 2021/public_test_question.json\"\n",
        "with open(data_path) as json_file:\n",
        "    questions = json.load(json_file)\n",
        "for question in questions['items']:\n",
        "    id = question['question_id']\n",
        "    q= question['question']\n",
        "    test.append([id, q])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIW1knF-hVvg",
        "outputId": "253c398d-f272-4b03-a705-4ad25dd12d01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n"
          ]
        }
      ],
      "source": [
        "to_save = []\n",
        "count = 0\n",
        "for question in test:\n",
        "    if count %50 == 0:\n",
        "        print(count)\n",
        "    count += 1\n",
        "    \n",
        "    k_docs = extract_top_k_for_testing(bm25, corpus_label, question[1], 10)\n",
        "    question_dict = {}\n",
        "    question_dict['question'] = question[1]\n",
        "    question_dict['id'] = question[0]\n",
        "    question_dict['relevant_articles'] = k_docs\n",
        "\n",
        "    to_save.append(question_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgvB488VhYat"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "import os\n",
        "dest_path = '/content/drive/MyDrive/Zalo 2021/Training Data'\n",
        "today = 'testing_bm25_top10_vers_short.json'\n",
        "\n",
        "file_path = os.path.join(dest_path, today)\n",
        "with codecs.open(file_path, \"w\", encoding='utf8') as outfile:\n",
        "        json.dump(to_save , outfile, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWzbcLkbiTAa"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BM25 Zalo 2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}